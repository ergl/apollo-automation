{run_template, "run.config.template"}.
% {run_template, "erlang_run.config.template"}.
{cluster_template, "cluster.config.template"}.

{load_configuration, #{
    key_limit => 1_000_000,
    %% size of value in bytes
    val_size => 256
}}.

%% Syntax for each experiment:
#{
    %% Number of threads per client machine.
    %% Allowed values are numbers (like `1`, `20`, etc), list of numbers,
    %% or a MFA ({M, F, A}) that in the end produces a generator of numbers
    %% For example, the line below produces the list [0, 50, 100]
    %% NOTE: The minimum number of workers is 1. All values less than 1 will be set to 1.
    clients => {lists, seq, [0, 100, 50]},

    %% The folder name under $HOME/results where all results for this test will go
    results_folder => "test_name",

    %% A map of template subsitutions.
    %% Check run.config.template for available values.
    %% In essence, anything present below will be changed in the corresponding
    %% run.config file supplied to the benchmark.
    %% Any key will be overriden, but the minimum amount of keys to be present
    %% is shown below.
    run_with => #{
        operations => [read],
        readonly_ops => 1,
        retry_aborts => false,
        %% Valid values: uniform, {uniform_exclude, Key}, pareto, {biased_key, Key, Bias}, {biased_key_worker_id, Key, Bias}, {constant_key, Key} Uniform is default if omitted
        %% For biased_key and biased_key_worker_id, bias is expressed as a percentage
        key_distribution => uniform
    },

    %% A map of config template subsitutions.
    %% Check cluster.config.template for available values.
    %% For the `clusters` key, you can either input a list of replica names,
    %% or a raw map like we used in old cluster.config files
    %%
    %% For the `leaders` key, you can put either a map from partition number
    %% to replica name, or a single replica name, in which case it will
    %% be used as the replica for every partition. If the `leaders` key is
    %% omitted, then a replica will be used at random.
    run_on => #{
        ext_tag => "release-X.Y.Z",
        clusters => [virginia],
        partitions => 1,
        %% This can be a number, or the `auto` atom, in which case it will
        %% allocate client machines automatically uniformly across all partitions,
        %% using all the available machines.
        per_partition => 4
    },

    %% When testing recovery, one can specify here a map of failure events
    %%
    %% Each replica can be paired with a (whole data center) failure, or
    %% failures to specific machines.
    %%
    %% - To specify a whole data center crash, use `replicaName => timeSpec`
    %% - To specify a specific partition, use `{replicaName, Partition} => timeSpec`
    failures_after => #{
        virginia => {minutes, 2},
        {california, 0} => {minutes, 2}
    },

    %% When using the crasher script, specify the arguments here
    %% -commitTimeout, -crashKey, -master_ip, -master_port, -opTimeout and -value_bytes
    %% will be derived from other parts of the configuration
    crasher_after => #{
        replica => virginia,
        hot_key => 0,
        at => {minutes, 2}
    }
}.

{experiments,
    [
        % Test crasher script
        #{
            results_folder => "0_7_1-spanner_crasher_test_p2_w1",
            clients => [0, 5, 10, 15, 20, 50],
            run_on => #{
                ext_tag => "release-0.7.1",
                leaders => virginia,
                clusters => [virginia, california, frankfurt],
                latencies => #{
                    virginia => [{california, 30}, {frankfurt, 30}],
                    california => [{virginia, 30}, {frankfurt, 30}],
                    frankfurt => [{virginia, 30}, {california, 30}]
                },
                partitions => 2,
                per_partition => 1,
                use_veleta => false,
                recovery_min_wait => {seconds, 5},
                prepare_retransmit_interval => {milliseconds, 130},
                commit_protocol => spanner,
                % No proactive aborts in place during recovery
                spanner_abort_interval => {minutes, 10},
                % Above load limit, we know no transaction will update this key
                contention_crash_key => 1_000_000
            },
            run_with => #{
                duration => 5,
                report_interval => {seconds, 1},
                key_range => 1000000,
                operations => [update],
                writeonly_ops => 1,
                retry_aborts => false,
                key_distribution => uniform,
                op_timeout => {seconds, 5},
                commit_timeout => {seconds, 5}
            },
            crasher_after => #{
                replica => virginia,
                hot_key => 1,
                at => {minutes, 2}
            }
        }
    ]
}.
